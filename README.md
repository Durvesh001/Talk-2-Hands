# Talk-2-Hands
Project Description:
Successfully developed and implemented a cutting-edge project named 'Talk 2 Hands,' focused on converting Indian Sign Language to text using advanced Machine Learning techniques. Our project entailed writing a comprehensive codebase, which incorporated key libraries such as OS, cvzone's Hand Tracking and Classifier Module, cv2, and numpy. Leveraging Transfer Learning on CNN, we meticulously trained a model utilizing Google Teachable Machine, using an extensive dataset of approximately 21,000 diverse sign language images. The primary objective of our project was to bridge the communication gap by accurately translating Indian Sign Language into text, thereby empowering individuals with hearing impairments. To achieve this, we adopted a rigorous training approach, enabling our model to achieve exceptional performance. The testing phase yielded an impressive accuracy rate of 80%, while our training accuracy reached a remarkable 100%.
To ensure the robustness and reliability of our solution, we meticulously curated a comprehensive testing dataset comprising 9,000 images representative of real-world sign language gestures. Through diligent training and testing, we validated the effectiveness of our approach and exhibited the potential for accurate and efficient sign language translation. By undertaking this project, we demonstrated our team's technical proficiency in handling complex challenges, applying advanced ML techniques, and utilizing libraries effectively. The Talk 2 Hands project stands as a testament to our dedication to innovation, accessibility, and inclusivity.

Following are the steps followed for making this project - 
1. 'data_collection.py' - code continuously tracks hands in video frames, extracts the hand region, and processes the hand images by resizing and pasting them onto a white background. The user can save the processed images into separate folders by pressing the corresponding keys on the keyboard.
2. 'Liveaction.py' - code demonstrates real-time sign language recognition by capturing video frames, detecting hands, processing the hand region, and classifying the sign language gesture using a pre-trained model. The predicted label is then displayed on the screen, enabling communication accessibility for individuals using sign language.
3. 'accuracy.py' - code calculates the accuracy of the testing dataset.

The project enables the conversion of Indian Sign Language to text using machine learning techniques. Trained the model with an extensive dataset of approximately 21,000 images, resulting in an impressive testing accuracy of 80% and a training accuracy of 100%. Demonstrated the potential for accurate and efficient translation of sign language to enhance communication accessibility.
